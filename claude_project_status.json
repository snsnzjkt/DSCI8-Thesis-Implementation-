{
  "timestamp": "2025-08-26T02:42:57.343424",
  "project_name": "SCS-ID Intrusion Detection",
  "structure": {
    "root": {
      "python_files": [
        "claude_commands.py",
        "config.py",
        "create_status.py"
      ],
      "other_files": [
        "claude_summary.md",
        "README.md",
        "requirements.txt"
      ],
      "directories": [
        "data",
        "experiments",
        "models"
      ]
    },
    "data": {
      "python_files": [
        "download_dataset.py",
        "preprocess.py"
      ],
      "other_files": [],
      "directories": []
    },
    "experiments": {
      "python_files": [
        "compare_models.py",
        "train_baseline.py",
        "train_scs_id.py"
      ],
      "other_files": [],
      "directories": []
    },
    "models": {
      "python_files": [
        "baseline_cnn.py",
        "scs_id.py",
        "utils.py"
      ],
      "other_files": [],
      "directories": []
    }
  },
  "files_content": {
    "config.py": {
      "content": "import os\nimport torch\n\nclass Config:\n    # Paths\n    DATA_DIR = \"data\"\n    RESULTS_DIR = \"results\"\n    \n    # Dataset\n    NUM_FEATURES = 78\n    SELECTED_FEATURES = 42\n    NUM_CLASSES = 16\n    \n    # Training\n    BATCH_SIZE = 32\n    LEARNING_RATE = 1e-4\n    EPOCHS = 25\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Model parameters\n    BASELINE_FILTERS = [120, 60, 30]\n    PRUNING_RATIO = 0.3\n    \n    # Create directories\n    os.makedirs(DATA_DIR, exist_ok=True)\n    os.makedirs(RESULTS_DIR, exist_ok=True)\n\nconfig = Config()\n",
      "size": 553,
      "lines": 28,
      "exists": true
    },
    "requirements.txt": {
      "content": "torch>=1.12.0\nscikit-learn>=1.1.0\npandas>=1.4.0\nnumpy>=1.21.0\nmatplotlib>=3.5.0\nseaborn>=0.11.0\njupyter>=1.0.0\nimbalanced-learn>=0.9.0\n",
      "size": 135,
      "lines": 8,
      "exists": true
    },
    "README.md": {
      "content": "# SCS-ID: A Squeezed ConvSeek for Efficient Intrusion Detection in Campus Networks\n\n## Quick Start\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Download and preprocess data\npython data/preprocess.py\n\n# Train baseline model\npython experiments/train_baseline.py\n\n# Train SCS-ID model\npython experiments/train_scs_id.py\n\n# Compare results\npython experiments/compare_models.py\n```\n\n## Team Members\n- Alba, Jomell Prinz E.\n- Dy, Gian Raphael C.\n- Esguerra, Edrine Frances A.\n- Gulifardo, Rayna Eliz P.\n\n",
      "size": 519,
      "lines": 26,
      "exists": true
    },
    "data/preprocess.py": {
      "content": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\nfrom imblearn.over_sampling import SMOTE\nimport pickle\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom config import config\n\nclass CICIDSPreprocessor:\n    \"\"\"Simple and efficient CIC-IDS2017 preprocessor\"\"\"\n    \n    def __init__(self):\n        self.scaler = StandardScaler()\n        self.label_encoder = LabelEncoder()\n        self.isolation_forest = IsolationForest(contamination=0.01, random_state=42)\n        \n    def load_data(self, file_path=None):\n        \"\"\"Load CIC-IDS2017 dataset\"\"\"\n        if file_path is None:\n            # Try common file locations\n            possible_paths = [\n                f\"{config.DATA_DIR}/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n                f\"{config.DATA_DIR}/Wednesday-workingHours.pcap_ISCX.csv\",\n                f\"{config.DATA_DIR}/cicids2017_sample.csv\"  # If you have a sample\n            ]\n            \n            for path in possible_paths:\n                if os.path.exists(path):\n                    file_path = path\n                    break\n            \n            if file_path is None:\n                # Create sample data for testing\n                return self._create_sample_data()\n        \n        print(f\"Loading data from: {file_path}\")\n        df = pd.read_csv(file_path)\n        \n        # Clean column names\n        df.columns = df.columns.str.strip()\n        \n        return df\n    \n    def _create_sample_data(self):\n        \"\"\"Create sample data for testing (remove this when you have real data)\"\"\"\n        print(\"‚ö†Ô∏è Creating sample data for testing...\")\n        print(\"   Replace this with actual CIC-IDS2017 data loading\")\n        \n        np.random.seed(42)\n        n_samples = 10000\n        n_features = 78\n        \n        # Create synthetic network traffic features\n        X = np.random.rand(n_samples, n_features)\n        \n        # Create attack labels (15 attack types + 1 benign)\n        attack_types = [\n            'BENIGN', 'DDoS', 'DoS GoldenEye', 'DoS Hulk', 'DoS Slowhttptest',\n            'DoS slowloris', 'FTP-Patator', 'SSH-Patator', 'Web Attack ‚Äì Brute Force',\n            'Web Attack ‚Äì Sql Injection', 'Web Attack ‚Äì XSS', 'Infiltration',\n            'Bot', 'PortScan', 'Heartbleed'\n        ]\n        \n        # Generate labels with realistic distribution (mostly benign)\n        labels = np.random.choice(attack_types, n_samples, \n                                p=[0.8] + [0.2/14]*14)  # 80% benign, 20% attacks\n        \n        # Create DataFrame\n        feature_names = [f'Feature_{i}' for i in range(n_features)]\n        df = pd.DataFrame(X, columns=feature_names)\n        df['Label'] = labels\n        \n        return df\n    \n    def clean_data(self, df):\n        \"\"\"Clean and prepare the dataset\"\"\"\n        print(\"üßπ Cleaning data...\")\n        \n        # Handle missing values\n        df = df.replace([np.inf, -np.inf], np.nan)\n        df = df.fillna(df.median(numeric_only=True))\n        \n        # Remove constant features\n        constant_cols = df.columns[df.nunique() <= 1]\n        if len(constant_cols) > 0:\n            print(f\"   Removing {len(constant_cols)} constant features\")\n            df = df.drop(columns=constant_cols)\n        \n        # Separate features and labels\n        X = df.drop('Label', axis=1)\n        y = df['Label']\n        \n        # Encode labels\n        y_encoded = self.label_encoder.fit_transform(y)\n        \n        print(f\"   Dataset shape: {X.shape}\")\n        print(f\"   Classes: {len(self.label_encoder.classes_)}\")\n        print(f\"   Class distribution: {np.bincount(y_encoded)}\")\n        \n        return X, y_encoded\n    \n    def normalize_features(self, X_train, X_test):\n        \"\"\"Z-score normalization\"\"\"\n        print(\"üìä Normalizing features...\")\n        \n        X_train_scaled = self.scaler.fit_transform(X_train)\n        X_test_scaled = self.scaler.transform(X_test)\n        \n        return X_train_scaled, X_test_scaled\n    \n    def remove_outliers(self, X, y):\n        \"\"\"Remove outliers using Isolation Forest\"\"\"\n        print(\"üîç Removing outliers...\")\n        \n        outlier_mask = self.isolation_forest.fit_predict(X) == 1\n        X_clean = X[outlier_mask]\n        y_clean = y[outlier_mask]\n        \n        removed = len(X) - len(X_clean)\n        print(f\"   Removed {removed} outliers ({removed/len(X)*100:.1f}%)\")\n        \n        return X_clean, y_clean\n    \n    def balance_classes(self, X, y):\n        \"\"\"Balance classes using SMOTE\"\"\"\n        print(\"‚öñÔ∏è Balancing classes...\")\n        \n        original_counts = np.bincount(y)\n        print(f\"   Original distribution: {original_counts}\")\n        \n        smote = SMOTE(random_state=42)\n        X_balanced, y_balanced = smote.fit_resample(X, y)\n        \n        balanced_counts = np.bincount(y_balanced)\n        print(f\"   Balanced distribution: {balanced_counts}\")\n        \n        return X_balanced, y_balanced\n    \n    def split_data(self, X, y):\n        \"\"\"Temporal split (70% train, 30% test)\"\"\"\n        print(\"‚úÇÔ∏è Splitting data...\")\n        \n        # For simplicity, use random split\n        # In real implementation, use temporal split based on timestamps\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.3, random_state=42, stratify=y\n        )\n        \n        print(f\"   Training set: {X_train.shape}\")\n        print(f\"   Test set: {X_test.shape}\")\n        \n        return X_train, X_test, y_train, y_test\n    \n    def preprocess_full_pipeline(self):\n        \"\"\"Complete preprocessing pipeline\"\"\"\n        print(\"üöÄ Starting CIC-IDS2017 preprocessing pipeline...\\n\")\n        \n        # Step 1: Load data\n        df = self.load_data()\n        \n        # Step 2: Clean data\n        X, y = self.clean_data(df)\n        \n        # Step 3: Split data first (before normalization)\n        X_train, X_test, y_train, y_test = self.split_data(X, y)\n        \n        # Step 4: Remove outliers from training data only\n        X_train_clean, y_train_clean = self.remove_outliers(X_train, y_train)\n        \n        # Step 5: Balance training data\n        X_train_balanced, y_train_balanced = self.balance_classes(X_train_clean, y_train_clean)\n        \n        # Step 6: Normalize features\n        X_train_final, X_test_final = self.normalize_features(X_train_balanced, X_test)\n        \n        # Step 7: Save processed data\n        self.save_processed_data(\n            X_train_final, X_test_final, \n            y_train_balanced, y_test\n        )\n        \n        print(\"\\n‚úÖ Preprocessing complete!\")\n        return X_train_final, X_test_final, y_train_balanced, y_test\n    \n    def save_processed_data(self, X_train, X_test, y_train, y_test):\n        \"\"\"Save processed data\"\"\"\n        print(\"üíæ Saving processed data...\")\n        \n        data = {\n            'X_train': X_train,\n            'X_test': X_test,\n            'y_train': y_train,\n            'y_test': y_test,\n            'label_encoder': self.label_encoder,\n            'scaler': self.scaler,\n            'feature_names': [f'feature_{i}' for i in range(X_train.shape[1])]\n        }\n        \n        with open(f\"{config.DATA_DIR}/processed_data.pkl\", 'wb') as f:\n            pickle.dump(data, f)\n        \n        print(f\"   Saved to: {config.DATA_DIR}/processed_data.pkl\")\n\ndef main():\n    \"\"\"Run preprocessing pipeline\"\"\"\n    preprocessor = CICIDSPreprocessor()\n    X_train, X_test, y_train, y_test = preprocessor.preprocess_full_pipeline()\n    \n    print(f\"\\nüìä Final dataset statistics:\")\n    print(f\"   Training features: {X_train.shape}\")\n    print(f\"   Test features: {X_test.shape}\")\n    print(f\"   Training labels: {y_train.shape}\")\n    print(f\"   Test labels: {y_test.shape}\")\n    print(f\"   Number of classes: {len(np.unique(y_train))}\")\n\nif __name__ == \"__main__\":\n    main()",
      "size": 7939,
      "lines": 222,
      "exists": true
    },
    "models/baseline_cnn.py": {
      "content": "import torch\nimport torch.nn as nn\nfrom config import Config\n\nclass BaselineCNN(nn.Module):\n    \"\"\"Ayeni et al. (2023) CNN Implementation\"\"\"\n    \n    def __init__(self):\n        super(BaselineCNN, self).__init__()\n        \n        # Reshape input for CNN (assuming 78 features -> 9x9 grid)\n        input_size = int(np.sqrt(Config.NUM_FEATURES)) + 1  # 9x9\n        \n        self.conv1 = nn.Conv2d(1, 120, kernel_size=2, padding='same')\n        self.conv2 = nn.Conv2d(120, 60, kernel_size=2, padding='same')\n        self.conv3 = nn.Conv2d(60, 30, kernel_size=2, padding='same')\n        \n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(30 * input_size * input_size, Config.NUM_CLASSES)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        # Reshape to 2D for CNN\n        batch_size = x.size(0)\n        x = x.view(batch_size, 1, 9, 9)  # Adjust based on your feature count\n        \n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        \n        x = self.flatten(x)\n        x = self.dropout(x)\n        x = self.fc(x)\n        \n        return x\n\ndef create_baseline_model():\n    \"\"\"Factory function to create baseline model\"\"\"\n    return BaselineCNN()",
      "size": 1289,
      "lines": 41,
      "exists": true
    },
    "experiments/train_baseline.py": {
      "content": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, accuracy_score\nimport time\n\nfrom config import config\n\nclass BaselineCNN(nn.Module):\n    \"\"\"Simplified baseline CNN for intrusion detection\"\"\"\n    \n    def __init__(self, input_features=78, num_classes=16):\n        super(BaselineCNN, self).__init__()\n        \n        # Since we have 1D features, we'll use 1D convolutions\n        self.conv1 = nn.Conv1d(1, 120, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(120, 60, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(60, 30, kernel_size=3, padding=1)\n        \n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.flatten = nn.Flatten()\n        \n        self.fc1 = nn.Linear(30, 64)\n        self.fc2 = nn.Linear(64, num_classes)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        # Reshape for 1D convolution: [batch, 1, features]\n        x = x.unsqueeze(1)\n        \n        # Convolutional layers\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        \n        # Global average pooling\n        x = self.pool(x)\n        x = self.flatten(x)\n        \n        # Fully connected layers\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\nclass BaselineTrainer:\n    \"\"\"Simple trainer for baseline model\"\"\"\n    \n    def __init__(self):\n        self.device = config.DEVICE\n        self.model = None\n        self.train_losses = []\n        self.train_accuracies = []\n        \n    def load_data(self):\n        \"\"\"Load preprocessed data\"\"\"\n        print(\"üìÇ Loading preprocessed data...\")\n        \n        with open(f\"{config.DATA_DIR}/processed_data.pkl\", 'rb') as f:\n            data = pickle.load(f)\n        \n        X_train = torch.FloatTensor(data['X_train'])\n        X_test = torch.FloatTensor(data['X_test'])\n        y_train = torch.LongTensor(data['y_train'])\n        y_test = torch.LongTensor(data['y_test'])\n        \n        print(f\"   Training data: {X_train.shape}\")\n        print(f\"   Test data: {X_test.shape}\")\n        \n        return X_train, X_test, y_train, y_test\n    \n    def create_data_loaders(self, X_train, X_test, y_train, y_test):\n        \"\"\"Create PyTorch data loaders\"\"\"\n        train_dataset = TensorDataset(X_train, y_train)\n        test_dataset = TensorDataset(X_test, y_test)\n        \n        train_loader = DataLoader(\n            train_dataset, \n            batch_size=config.BATCH_SIZE, \n            shuffle=True\n        )\n        test_loader = DataLoader(\n            test_dataset, \n            batch_size=config.BATCH_SIZE, \n            shuffle=False\n        )\n        \n        return train_loader, test_loader\n    \n    def train_epoch(self, model, train_loader, criterion, optimizer):\n        \"\"\"Train for one epoch\"\"\"\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch_x, batch_y in train_loader:\n            batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            outputs = model(batch_x)\n            loss = criterion(outputs, batch_y)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += batch_y.size(0)\n            correct += (predicted == batch_y).sum().item()\n        \n        epoch_loss = total_loss / len(train_loader)\n        epoch_acc = 100 * correct / total\n        \n        return epoch_loss, epoch_acc\n    \n    def evaluate(self, model, test_loader):\n        \"\"\"Evaluate model on test set\"\"\"\n        model.eval()\n        all_predictions = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch_x, batch_y in test_loader:\n                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n                \n                outputs = model(batch_x)\n                _, predicted = torch.max(outputs, 1)\n                \n                all_predictions.extend(predicted.cpu().numpy())\n                all_labels.extend(batch_y.cpu().numpy())\n        \n        accuracy = accuracy_score(all_labels, all_predictions)\n        return accuracy, all_predictions, all_labels\n    \n    def train_model(self):\n        \"\"\"Complete training pipeline\"\"\"\n        print(\"üöÄ Starting baseline CNN training...\\n\")\n        \n        # Load data\n        X_train, X_test, y_train, y_test = self.load_data()\n        train_loader, test_loader = self.create_data_loaders(X_train, X_test, y_train, y_test)\n        \n        # Create model\n        num_classes = len(torch.unique(y_train))\n        input_features = X_train.shape[1]\n        \n        self.model = BaselineCNN(input_features, num_classes).to(self.device)\n        \n        print(f\"üß† Model created:\")\n        print(f\"   Input features: {input_features}\")\n        print(f\"   Output classes: {num_classes}\")\n        print(f\"   Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n        print(f\"   Device: {self.device}\\n\")\n        \n        # Training setup\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.model.parameters(), lr=config.LEARNING_RATE)\n        \n        # Training loop\n        print(\"üèãÔ∏è Training...\")\n        start_time = time.time()\n        \n        for epoch in range(config.EPOCHS):\n            train_loss, train_acc = self.train_epoch(\n                self.model, train_loader, criterion, optimizer\n            )\n            \n            self.train_losses.append(train_loss)\n            self.train_accuracies.append(train_acc)\n            \n            if (epoch + 1) % 5 == 0 or epoch == 0:\n                print(f\"   Epoch {epoch+1:2d}/{config.EPOCHS}: \"\n                      f\"Loss={train_loss:.4f}, Accuracy={train_acc:.2f}%\")\n        \n        training_time = time.time() - start_time\n        print(f\"\\n‚è±Ô∏è Training completed in {training_time:.2f} seconds\")\n        \n        # Evaluation\n        print(\"\\nüìä Evaluating model...\")\n        test_accuracy, predictions, labels = self.evaluate(self.model, test_loader)\n        \n        print(f\"üéØ Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n        \n        # Save results\n        self.save_results(test_accuracy, predictions, labels, training_time)\n        \n        return self.model, test_accuracy\n    \n    def save_results(self, test_accuracy, predictions, labels, training_time):\n        \"\"\"Save model and results\"\"\"\n        print(\"\\nüíæ Saving results...\")\n        \n        # Save model\n        torch.save(self.model.state_dict(), f\"{config.RESULTS_DIR}/baseline_model.pth\")\n        \n        # Save metrics\n        results = {\n            'test_accuracy': test_accuracy,\n            'training_time': training_time,\n            'train_losses': self.train_losses,\n            'train_accuracies': self.train_accuracies,\n            'predictions': predictions,\n            'labels': labels,\n            'model_parameters': sum(p.numel() for p in self.model.parameters())\n        }\n        \n        with open(f\"{config.RESULTS_DIR}/baseline_results.pkl\", 'wb') as f:\n            pickle.dump(results, f)\n        \n        # Plot training curves\n        self.plot_training_curves()\n        \n        print(f\"   Model saved: {config.RESULTS_DIR}/baseline_model.pth\")\n        print(f\"   Results saved: {config.RESULTS_DIR}/baseline_results.pkl\")\n    \n    def plot_training_curves(self):\n        \"\"\"Plot and save training curves\"\"\"\n        plt.figure(figsize=(12, 4))\n        \n        plt.subplot(1, 2, 1)\n        plt.plot(self.train_losses)\n        plt.title('Training Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.grid(True)\n        \n        plt.subplot(1, 2, 2)\n        plt.plot(self.train_accuracies)\n        plt.title('Training Accuracy')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy (%)')\n        plt.grid(True)\n        \n        plt.tight_layout()\n        plt.savefig(f\"{config.RESULTS_DIR}/baseline_training_curves.png\")\n        plt.close()\n\ndef main():\n    \"\"\"Run baseline training\"\"\"\n    trainer = BaselineTrainer()\n    model, accuracy = trainer.train_model()\n    \n    print(f\"\\n‚úÖ Baseline training complete!\")\n    print(f\"üèÜ Final test accuracy: {accuracy:.4f}\")\n    print(f\"üìÅ Results saved in: {config.RESULTS_DIR}/\")\n\nif __name__ == \"__main__\":\n    main()",
      "size": 8695,
      "lines": 259,
      "exists": true
    },
    "models/scs_id.py": {
      "content": "",
      "size": 0,
      "lines": 0,
      "exists": true
    }
  },
  "recent_outputs": {
    "data": [
      {
        "name": "download_dataset.py",
        "modified": "2025-08-26T02:34:12.777516",
        "size": 0
      },
      {
        "name": "preprocess.py",
        "modified": "2025-08-26T02:34:12.777516",
        "size": 8198
      }
    ]
  },
  "environment": {
    "python_version": "3.11.9",
    "key_packages": {
      "torch": "Not installed",
      "scikit-learn": "Not installed",
      "pandas": "Not installed",
      "numpy": "Not installed",
      "matplotlib": "Not installed"
    },
    "working_directory": "C:\\Users\\Kate\\DSCI8-Thesis-Implementation-"
  },
  "git_status": {
    "current_branch": "main",
    "status": "?? .vscode/\n?? claude_commands.py\n?? claude_project_status.json\n?? claude_summary.md\n?? create_status.py",
    "has_changes": true
  },
  "todo": [
    {
      "file": ".\\create_status.py",
      "line": 176,
      "content": "\"\"\"Extract TODO items from code\"\"\""
    },
    {
      "file": ".\\create_status.py",
      "line": 190,
      "content": "if 'TODO' in line or 'FIXME' in line or 'BUG' in line:"
    },
    {
      "file": ".\\create_status.py",
      "line": 242,
      "content": "summary += f\"\\n## TODO Items ({len(status['todo'])} found)\\n\""
    }
  ]
}
