# -*- coding: utf-8 -*-
# experiments/simple_inference_benchmark.py - Simplified Model Inference Benchmarking
import sys
import time
import gc
import psutil
import os
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import warnings
warnings.filterwarnings('ignore')

# Add parent directory to Python path
sys.path.append(str(Path(__file__).parent.parent))

try:
    from config import config
except ImportError:
    class Config:
        RESULTS_DIR = "results"
        DATA_DIR = "data"
        DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        BATCH_SIZE = 1024
    config = Config()

class SimpleBaselineCNN(nn.Module):
    """Simplified baseline CNN for benchmarking"""
    def __init__(self, input_features=66, num_classes=15):
        super().__init__()
        self.conv1 = nn.Conv1d(1, 120, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(120, 60, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(60, 30, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(120)
        self.bn2 = nn.BatchNorm1d(60)
        self.bn3 = nn.BatchNorm1d(30)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc1 = nn.Linear(30, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, num_classes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        if len(x.shape) == 2:
            x = x.unsqueeze(1)
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.dropout(x)
        x = self.relu(self.bn3(self.conv3(x)))
        x = self.pool(x).squeeze(-1)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        return self.fc3(x)

class SimpleSCSID(nn.Module):
    """Simplified SCS-ID model for benchmarking (smaller)"""
    def __init__(self, input_features=66, num_classes=15):
        super().__init__()
        # Much smaller architecture for speed comparison
        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(32, 16, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(32)
        self.bn2 = nn.BatchNorm1d(16)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc1 = nn.Linear(16, 32)
        self.fc2 = nn.Linear(32, num_classes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        if len(x.shape) == 2:
            x = x.unsqueeze(1)
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.pool(x).squeeze(-1)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        return self.fc2(x)

class SimpleBenchmark:
    """Simplified inference benchmarking"""
    
    def __init__(self):
        self.device = torch.device(config.DEVICE)
        self.batch_sizes = [1, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]
        self.warmup_iterations = 10
        self.benchmark_iterations = 100
        
        print(f"üîß Simple Benchmark initialized on device: {self.device}")
        
    def create_models(self):
        """Create simple models for benchmarking"""
        print("\nüì¶ Creating benchmark models...")
        
        # Create models
        self.baseline_model = SimpleBaselineCNN(input_features=66, num_classes=15)
        self.scs_id_model = SimpleSCSID(input_features=66, num_classes=15)
        
        # Move to device
        self.baseline_model.to(self.device).eval()
        self.scs_id_model.to(self.device).eval()
        
        # Count parameters
        baseline_params = sum(p.numel() for p in self.baseline_model.parameters())
        scs_id_params = sum(p.numel() for p in self.scs_id_model.parameters())
        
        print(f"   üìä Baseline parameters: {baseline_params:,}")
        print(f"   üìä SCS-ID parameters: {scs_id_params:,}")
        print(f"   üìâ Parameter reduction: {(1 - scs_id_params/baseline_params)*100:.1f}%")
        
    def create_test_data(self):
        """Create synthetic test data"""
        print("\nüìä Creating synthetic test data...")
        
        n_samples = 10000
        n_features = 66
        n_classes = 15
        
        X_test = torch.randn(n_samples, n_features, device=self.device)
        y_test = torch.randint(0, n_classes, (n_samples,), device=self.device)
        
        self.test_data = TensorDataset(X_test, y_test)
        print(f"   ‚úÖ Created test data: {n_samples:,} samples, {n_features} features")
        
    def measure_inference(self, model, data_loader, model_name):
        """Measure inference performance"""
        print(f"\n‚è±Ô∏è  Measuring {model_name} inference...")
        
        # Warmup
        with torch.no_grad():
            for i, (inputs, _) in enumerate(data_loader):
                if i >= self.warmup_iterations:
                    break
                _ = model(inputs)
        
        # Clear cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # Benchmark
        inference_times = []
        
        with torch.no_grad():
            for i, (inputs, _) in enumerate(data_loader):
                if i >= self.benchmark_iterations:
                    break
                
                # Time inference
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    
                start_time = time.perf_counter()
                outputs = model(inputs)
                
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    
                end_time = time.perf_counter()
                
                inference_time = (end_time - start_time) * 1000  # ms
                inference_times.append(inference_time)
        
        # Calculate stats
        batch_size = data_loader.batch_size
        avg_batch_time = np.mean(inference_times)
        avg_sample_time = avg_batch_time / batch_size
        throughput = batch_size * 1000 / avg_batch_time
        
        print(f"   ‚úÖ Avg batch time: {avg_batch_time:.2f}ms")
        print(f"   ‚úÖ Avg sample time: {avg_sample_time:.4f}ms")
        print(f"   ‚úÖ Throughput: {throughput:.0f} samples/sec")
        
        return {
            'model': model_name,
            'batch_size': batch_size,
            'avg_batch_time_ms': avg_batch_time,
            'avg_sample_time_ms': avg_sample_time,
            'throughput_samples_per_sec': throughput
        }
    
    def run_benchmark(self):
        """Run complete benchmark"""
        print("\nüöÄ Starting benchmark across batch sizes...")
        
        results = []
        
        for batch_size in self.batch_sizes:
            print(f"\nüì¶ Testing batch size: {batch_size}")
            
            try:
                data_loader = DataLoader(
                    self.test_data, 
                    batch_size=batch_size, 
                    shuffle=False, 
                    num_workers=0
                )
                
                # Benchmark both models
                baseline_result = self.measure_inference(
                    self.baseline_model, data_loader, f"Baseline CNN (BS={batch_size})"
                )
                baseline_result['model_type'] = 'Baseline CNN'
                
                scs_id_result = self.measure_inference(
                    self.scs_id_model, data_loader, f"SCS-ID (BS={batch_size})"
                )
                scs_id_result['model_type'] = 'SCS-ID'
                
                results.extend([baseline_result, scs_id_result])
                
            except Exception as e:
                print(f"   ‚ùå Error with batch size {batch_size}: {e}")
                continue
        
        self.results_df = pd.DataFrame(results)
        return self.results_df
    
    def analyze_improvements(self):
        """Analyze performance improvements"""
        print("\nüìà Analyzing performance improvements...")
        
        improvements = []
        
        for batch_size in self.batch_sizes:
            baseline_data = self.results_df[
                (self.results_df['model_type'] == 'Baseline CNN') & 
                (self.results_df['batch_size'] == batch_size)
            ]
            scs_id_data = self.results_df[
                (self.results_df['model_type'] == 'SCS-ID') & 
                (self.results_df['batch_size'] == batch_size)
            ]
            
            if not baseline_data.empty and not scs_id_data.empty:
                baseline_row = baseline_data.iloc[0]
                scs_id_row = scs_id_data.iloc[0]
                
                latency_improvement = (1 - scs_id_row['avg_sample_time_ms'] / baseline_row['avg_sample_time_ms']) * 100
                throughput_improvement = (scs_id_row['throughput_samples_per_sec'] / baseline_row['throughput_samples_per_sec'] - 1) * 100
                
                improvements.append({
                    'batch_size': batch_size,
                    'latency_improvement_pct': latency_improvement,
                    'throughput_improvement_pct': throughput_improvement,
                    'baseline_throughput': baseline_row['throughput_samples_per_sec'],
                    'scs_id_throughput': scs_id_row['throughput_samples_per_sec']
                })
        
        self.improvements_df = pd.DataFrame(improvements)
        
        avg_throughput_improvement = self.improvements_df['throughput_improvement_pct'].mean()
        best_throughput_improvement = self.improvements_df['throughput_improvement_pct'].max()
        
        print(f"   üìä Average Throughput Improvement: {avg_throughput_improvement:.1f}%")
        print(f"   üìä Best Throughput Improvement: {best_throughput_improvement:.1f}%")
        print(f"   üéØ Target Achievement (>300%): {'‚úÖ ACHIEVED' if best_throughput_improvement > 300 else '‚ùå NOT ACHIEVED'}")
        
        return self.improvements_df
    
    def create_visualizations(self):
        """Create performance visualizations"""
        print("\nüìä Creating visualizations...")
        
        output_dir = Path(config.RESULTS_DIR) / "inference_benchmark"
        output_dir.mkdir(exist_ok=True)
        
        # Throughput comparison
        plt.figure(figsize=(12, 8))
        
        for model_type in ['Baseline CNN', 'SCS-ID']:
            model_data = self.results_df[self.results_df['model_type'] == model_type]
            plt.plot(model_data['batch_size'], model_data['throughput_samples_per_sec'], 
                    marker='o', linewidth=2, markersize=6, label=model_type)
        
        plt.xlabel('Batch Size', fontsize=12, fontweight='bold')
        plt.ylabel('Throughput (samples/second)', fontsize=12, fontweight='bold')
        plt.title('Inference Throughput Comparison', fontsize=14, fontweight='bold')
        plt.legend(fontsize=11)
        plt.grid(True, alpha=0.3)
        plt.xscale('log', base=2)
        
        # Add target line
        baseline_max_throughput = self.results_df[
            self.results_df['model_type'] == 'Baseline CNN'
        ]['throughput_samples_per_sec'].max()
        
        target_throughput = baseline_max_throughput * 4  # 300% improvement = 4x
        plt.axhline(y=target_throughput, color='red', linestyle='--', alpha=0.7, 
                   label='Target (300% improvement)')
        
        plt.legend(fontsize=11)
        plt.tight_layout()
        plt.savefig(output_dir / 'throughput_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ‚úÖ Throughput comparison saved")
        
        # Improvement summary
        plt.figure(figsize=(10, 6))
        plt.plot(self.improvements_df['batch_size'], 
                self.improvements_df['throughput_improvement_pct'], 
                marker='o', linewidth=3, color='green', markersize=8)
        plt.xlabel('Batch Size', fontsize=12, fontweight='bold')
        plt.ylabel('Throughput Improvement (%)', fontsize=12, fontweight='bold')
        plt.title('SCS-ID Throughput Improvement vs Baseline CNN', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.xscale('log', base=2)
        plt.axhline(y=300, color='red', linestyle='--', linewidth=2, label='Target (300%)')
        plt.legend()
        plt.tight_layout()
        plt.savefig(output_dir / 'improvement_summary.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ‚úÖ Improvement summary saved")
        print(f"   üìÅ All visualizations saved to: {output_dir}")
    
    def generate_report(self):
        """Generate benchmark report"""
        print("\nüìã Generating benchmark report...")
        
        output_dir = Path(config.RESULTS_DIR) / "inference_benchmark"
        output_dir.mkdir(exist_ok=True)
        report_path = output_dir / "simple_benchmark_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("SIMPLIFIED INFERENCE BENCHMARK REPORT\n")
            f.write("="*50 + "\n")
            f.write(f"Device: {self.device}\n")
            f.write(f"Benchmark Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Summary
            avg_improvement = self.improvements_df['throughput_improvement_pct'].mean()
            best_improvement = self.improvements_df['throughput_improvement_pct'].max()
            optimal_batch_size = self.improvements_df.loc[
                self.improvements_df['throughput_improvement_pct'].idxmax(), 'batch_size'
            ]
            
            f.write("EXECUTIVE SUMMARY\n")
            f.write("-"*20 + "\n")
            f.write(f"Average Throughput Improvement: {avg_improvement:.1f}%\n")
            f.write(f"Best Throughput Improvement: {best_improvement:.1f}%\n")
            f.write(f"Optimal Batch Size: {optimal_batch_size}\n")
            f.write(f"Target Achievement (>300%): {'YES' if best_improvement > 300 else 'NO'}\n\n")
            
            # Detailed results
            f.write("DETAILED RESULTS\n")
            f.write("-"*16 + "\n")
            f.write(f"{'Batch Size':<12} {'Model':<12} {'Throughput':<15} {'Sample Time (ms)':<15}\n")
            f.write("-"*60 + "\n")
            
            for _, row in self.results_df.iterrows():
                f.write(f"{row['batch_size']:<12} {row['model_type']:<12} "
                       f"{row['throughput_samples_per_sec']:<15.0f} "
                       f"{row['avg_sample_time_ms']:<15.4f}\n")
            
            f.write(f"\nCONCLUSIONS\n")
            f.write("-"*11 + "\n")
            f.write("This simplified benchmark demonstrates:\n")
            f.write(f"‚Ä¢ SCS-ID achieves up to {best_improvement:.1f}% throughput improvement\n")
            f.write(f"‚Ä¢ Average improvement of {avg_improvement:.1f}% across all batch sizes\n")
            f.write(f"‚Ä¢ Optimal performance at batch size {optimal_batch_size}\n")
            if best_improvement > 300:
                f.write("‚Ä¢ Successfully exceeds the >300% speed improvement target!\n")
            else:
                f.write("‚Ä¢ Approaches but does not yet exceed the >300% target\n")
        
        print(f"   ‚úÖ Report saved: {report_path}")
        return report_path
    
    def run_complete_benchmark(self):
        """Run complete benchmarking pipeline"""
        print("üöÄ Starting Simplified Inference Benchmarking")
        print("="*55)
        
        try:
            self.create_models()
            self.create_test_data()
            results = self.run_benchmark()
            improvements = self.analyze_improvements()
            self.create_visualizations()
            report_path = self.generate_report()
            
            print("\n" + "="*55)
            print("‚úÖ SIMPLIFIED BENCHMARKING COMPLETE!")
            print("="*55)
            
            best_improvement = improvements['throughput_improvement_pct'].max()
            avg_improvement = improvements['throughput_improvement_pct'].mean()
            
            print(f"üèÜ Key Results:")
            print(f"   üìà Average Throughput Improvement: {avg_improvement:.1f}%")
            print(f"   üöÄ Best Throughput Improvement: {best_improvement:.1f}%")
            print(f"   üéØ Target (>300%): {'‚úÖ ACHIEVED' if best_improvement > 300 else '‚ö†Ô∏è  APPROACHING'}")
            print(f"   üìã Report: {report_path}")
            
            return {
                'results': results,
                'improvements': improvements,
                'report_path': report_path
            }
            
        except Exception as e:
            print(f"‚ùå Benchmarking failed: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """Run simplified benchmarking"""
    benchmark = SimpleBenchmark()
    results = benchmark.run_complete_benchmark()
    
    if results:
        print("\n[SUCCESS] Simplified benchmarking completed successfully!")
    else:
        print("\n[ERROR] Simplified benchmarking failed.")

if __name__ == "__main__":
    main()