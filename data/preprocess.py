# data/preprocess.py - Enhanced CIC-IDS2017 preprocessing with visualizations and data leakage checks
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from imblearn.over_sampling import SMOTE
from typing import Dict, Union
import pickle
import os
import glob
import warnings
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.metrics import mutual_info_score
from scipy import stats
from datetime import datetime
import hashlib
warnings.filterwarnings('ignore')

# Set up plotting
plt.style.use('default')
sns.set_palette("husl")
fig_size = (12, 8)

try:
    from config import config
except ImportError:
    # Fallback config if config.py doesn't exist
    class Config:
        DATA_DIR = "data"
        RESULTS_DIR = "results"
        VISUALIZATIONS_DIR = "visualizations"
        NUM_FEATURES = 78  # Keep all 78 original features
        ORIGINAL_FEATURES = 78  # Original feature count
        SELECTED_FEATURES = 42
        PRESERVE_ALL_FEATURES = True  # Preserve all features even if unusable
        USE_SINGLE_COLOR = True  # Use single color for visualizations
        SEPARATE_VISUALIZATIONS = True  # Create separate graphs for raw and preprocessed data
        NUM_CLASSES = 15  # 15 attack types: BENIGN + 14 attack classes
        BATCH_SIZE = 64
        DEVICE = "cpu"
        
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(RESULTS_DIR, exist_ok=True)
        os.makedirs(VISUALIZATIONS_DIR, exist_ok=True)
    
    config = Config()

class CICIDSPreprocessor:
    """CIC-IDS2017 preprocessor that handles files WITHOUT label columns"""
    
    def __init__(self):
        self.raw_dir = Path(config.DATA_DIR) / "raw"
        self.processed_dir = Path(config.DATA_DIR) / "processed"
        self.viz_dir = Path(config.VISUALIZATIONS_DIR)
        
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.isolation_forest = IsolationForest(contamination=0.01, random_state=42)
        self.imputer = SimpleImputer(strategy='median')
        
        # Ensure directories exist
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        self.viz_dir.mkdir(parents=True, exist_ok=True)
        
        # Keep all original features - no removal list needed
        # We'll only remove truly constant features (nunique = 1) if any exist
        self.preserve_all_features = True
        self.force_preserve_all_78 = True  # Force preservation of all 78 features
        self.primary_color = '#2E86AB'  # Professional blue for all visualizations
        
        # CIC-IDS2017 filename to attack type mapping
        self.filename_to_attacks = {
            'Monday': {
                'attacks': ['BENIGN'],
                'distribution': [1.0]  # 100% benign
            },
            'Tuesday': {
                'attacks': ['BENIGN', 'FTP-Patator', 'SSH-Patator'],
                'distribution': [0.85, 0.075, 0.075]  # Mostly benign with some attacks
            },
            'Wednesday': {
                'attacks': ['BENIGN', 'DoS GoldenEye', 'DoS Hulk', 'DoS Slowhttptest', 'DoS slowloris', 'Heartbleed'],
                'distribution': [0.67, 0.06, 0.15, 0.06, 0.06, 0.01]  # DoS heavy day - Fixed to sum to 1.01 ‚âà 1.0
            },
            'Thursday-Morning-WebAttacks': {
                'attacks': ['BENIGN', 'Web Attack - Brute Force', 'Web Attack - XSS', 'Web Attack - Sql Injection'],
                'distribution': [0.8, 0.07, 0.07, 0.06]  # Web attacks
            },
            'Thursday-Afternoon-Infilteration': {
                'attacks': ['BENIGN', 'Infiltration'],
                'distribution': [0.99, 0.01]  # Very few infiltration attacks
            },
            'Friday-Morning': {
                'attacks': ['BENIGN', 'Bot'],
                'distribution': [0.8, 0.2]  # Bot activity
            },
            'Friday-Afternoon-PortScan': {
                'attacks': ['BENIGN', 'PortScan'],
                'distribution': [0.85, 0.15]  # Port scanning
            },
            'Friday-Afternoon-DDos': {
                'attacks': ['BENIGN', 'DDoS'],
                'distribution': [0.8, 0.2]  # DDoS attacks
            }
        }
        
        self.all_attack_types = [
            'BENIGN', 'DDoS', 'DoS GoldenEye', 'DoS Hulk', 'DoS Slowhttptest',
            'DoS slowloris', 'FTP-Patator', 'SSH-Patator', 'Web Attack - Brute Force',
            'Web Attack - Sql Injection', 'Web Attack - XSS', 'Infiltration',
            'Bot', 'PortScan', 'Heartbleed'
        ]
    
    def create_before_preprocessing_visualizations(self, df):
        """Create separate raw data overview visualizations with single color"""
        print("üìä Creating raw data overview visualization...")
        
        # 1. Raw Data Overview - Class Distribution (separate graph)
        plt.figure(figsize=(12, 8))
        label_col = 'Label' if 'Label' in df.columns else ' Label'
        if label_col in df.columns:
            label_counts = df[label_col].value_counts()
            
            # Create horizontal bar chart with single color
            y_pos = np.arange(len(label_counts))
            plt.barh(y_pos, label_counts.values, color=self.primary_color)
            plt.yticks(y_pos, label_counts.index, fontsize=9)
            plt.xlabel('Sample Count')
            plt.title('Raw Data Overview: Attack Class Distribution', fontsize=16, fontweight='bold')
            plt.xscale('log')  # Log scale due to severe imbalance
            
            # Add percentage labels
            total_samples = len(df)
            for i, v in enumerate(label_counts.values):
                percentage = (v / total_samples) * 100
                plt.text(v, i, f' {percentage:.1f}%', va='center', fontsize=8)
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'raw_data_overview.png', dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # 2. Data Quality Overview (separate graph)
        plt.figure(figsize=(12, 8))
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols if col.strip().lower() != 'label']
        
        # Count missing values
        missing_counts = df[feature_cols].isnull().sum()
        missing_features = missing_counts[missing_counts > 0]
        
        # Count infinite values
        inf_counts = {}
        for col in feature_cols:
            inf_count = np.isinf(df[col]).sum()
            if inf_count > 0:
                inf_counts[col] = inf_count
        
        # Data quality bar chart with single color
        if len(missing_features) > 0:
            top_missing = missing_features.head(10)
            plt.bar(range(len(top_missing)), top_missing.values, color=self.primary_color)
            plt.xticks(range(len(top_missing)), [col[:15] for col in top_missing.index], rotation=45)
            plt.xlabel('Features')
            plt.ylabel('Missing Count')
            plt.title('Raw Data Quality: Missing Values by Feature (Top 10)', fontsize=16, fontweight='bold')
        else:
            plt.text(0.5, 0.5, 'No Missing Values Found', ha='center', va='center', fontsize=16, 
                    transform=plt.gca().transAxes)
            plt.title('Raw Data Quality: No Missing Values', fontsize=16, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'raw_data_quality.png', dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"   ‚úÖ Saved raw data overview visualizations to {self.viz_dir}")
    
    def create_after_preprocessing_visualizations(self, X_train, X_test, y_train, y_test, feature_names):
        """Create separate preprocessed data visualizations with single color"""
        print("üìä Creating after preprocessing visualizations...")
        
        # 1. Preprocessed Data Overview - Class Distribution (separate graph)
        plt.figure(figsize=(12, 8))
        unique_train, counts_train = np.unique(y_train, return_counts=True)
        class_names = [str(self.label_encoder.classes_[i]) for i in unique_train]
        
        # Create horizontal bar chart with single color
        y_positions = np.arange(len(unique_train))
        bars = plt.barh(y_positions, counts_train, color=self.primary_color)
        
        plt.yticks(y_positions, class_names, fontsize=9)
        plt.xlabel('Sample Count')
        plt.title('Preprocessed Data Overview: Training Set Class Distribution (After SMOTE 1:1 Balance)', 
                 fontsize=16, fontweight='bold')
        
        # Add count labels on bars
        for i, (bar, count) in enumerate(zip(bars, counts_train)):
            plt.text(count + max(counts_train) * 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{count:,}', ha='left', va='center', fontsize=8)
        
        plt.grid(True, alpha=0.3, axis='x')
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'preprocessed_data_overview.png', dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # 2. Dataset Statistics (separate graph)
        plt.figure(figsize=(12, 8))
        stats_info = [
            f'Final Features: {X_train.shape[1]}',
            f'Training Samples: {len(X_train):,}',
            f'Test Samples: {len(X_test):,}', 
            f'Classes: {len(np.unique(y_train))}',
            f'Feature Range: [{X_train.min():.2f}, {X_train.max():.2f}]',
            f'Train-Test Split: {len(X_train)/(len(X_train)+len(X_test)):.1%} - {len(X_test)/(len(X_train)+len(X_test)):.1%}'
        ]
        
        plt.text(0.1, 0.9, '\n'.join(stats_info), transform=plt.gca().transAxes, 
                fontsize=14, verticalalignment='top', fontfamily='monospace')
        plt.title('Preprocessed Dataset Statistics', fontsize=16, fontweight='bold')
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'preprocessed_dataset_stats.png', dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # 2. Feature importance visualization (using mutual information)
        print("   Computing feature importance...")
        # Use a sample for efficiency
        sample_size = min(5000, len(X_train))
        sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
        X_sample = X_train[sample_indices]
        y_sample = y_train[sample_indices]
        
        # Compute mutual information scores
        mi_scores = []
        for i in range(X_sample.shape[1]):
            try:
                score = mutual_info_score(X_sample[:, i], y_sample)
                mi_scores.append(score)
            except:
                mi_scores.append(0)
        
        # Plot top features with single color
        feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': mi_scores
        }).sort_values('importance', ascending=False)
        
        plt.figure(figsize=(12, 8))
        top_features = feature_importance.head(20)
        plt.barh(range(len(top_features)), top_features['importance'], color=self.primary_color)
        plt.yticks(range(len(top_features)), top_features['feature'].tolist())
        plt.xlabel('Mutual Information Score')
        plt.title('Top 20 Most Important Features (Mutual Information)', fontsize=16, fontweight='bold')
        plt.gca().invert_yaxis()
        plt.grid(True, alpha=0.3, axis='x')
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'feature_importance.png', dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"   ‚úÖ Saved processed data visualizations to {self.viz_dir}")
    
    def create_imputation_validation_plots(self, original_data, imputed_data, feature_names):
        """Create plots to validate imputation accuracy"""
        print("üìä Creating imputation validation plots...")
        
        # Find features that had missing values
        missing_features = []
        for col in original_data.columns:
            if original_data[col].isnull().sum() > 0:
                missing_features.append(col)
        
        if not missing_features:
            print("   No missing values found to validate imputation")
            return
        
        # Create comparison plots for features with missing values
        n_features = min(6, len(missing_features))
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Imputation Validation: Original vs Imputed Data Distributions', fontsize=16, fontweight='bold')
        
        for i, feature in enumerate(missing_features[:n_features]):
            row, col = i // 3, i % 3
            
            # Get original non-missing data
            original_clean = original_data[feature].dropna()
            
            # Get imputed data for the same feature
            if feature in feature_names:
                feature_idx = feature_names.index(feature)
                imputed_values = imputed_data[:, feature_idx]
                
                # Plot distributions
                axes[row, col].hist(original_clean, bins=30, alpha=0.7, label='Original (non-missing)', density=True)
                axes[row, col].hist(imputed_values, bins=30, alpha=0.7, label='After Imputation', density=True)
                axes[row, col].set_title(f'{feature}')
                axes[row, col].set_xlabel('Value')
                axes[row, col].set_ylabel('Density')
                axes[row, col].legend()
        
        # Fill empty subplots
        for i in range(n_features, 6):
            row, col = i // 3, i % 3
            axes[row, col].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'imputation_validation.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ‚úÖ Saved imputation validation plots to {self.viz_dir}")
    
    def detect_data_leakage(self, X_train, X_test, y_train, y_test, feature_names):
        """Comprehensive data leakage detection"""
        print("üîç Detecting potential data leakage...")
        
        leakage_results = {
            'train_test_contamination': False,
            'temporal_leakage': False,
            'normalization_leakage': False,
            'feature_leakage': [],
            'details': []
        }
        
        # 1. Check for identical samples between train and test (train-test contamination)
        print("   Checking for train-test contamination...")
        train_hashes = set()
        for i in range(len(X_train)):
            # Create hash of each training sample
            sample_hash = hashlib.md5(X_train[i].tobytes()).hexdigest()
            train_hashes.add(sample_hash)
        
        contamination_count = 0
        for i in range(len(X_test)):
            sample_hash = hashlib.md5(X_test[i].tobytes()).hexdigest()
            if sample_hash in train_hashes:
                contamination_count += 1
        
        if contamination_count > 0:
            leakage_results['train_test_contamination'] = True
            leakage_results['details'].append(f"Found {contamination_count} identical samples between train and test sets")
            print(f"   ‚ö†Ô∏è  LEAKAGE DETECTED: {contamination_count} identical samples in train and test sets")
        else:
            print("   ‚úÖ No train-test contamination detected")
        
        # 2. Check for features that perfectly separate classes (potential feature leakage)
        print("   Checking for feature leakage...")
        for i, feature_name in enumerate(feature_names):
            # Check if any feature has perfect correlation with labels
            feature_values = X_train[:, i]
            
            # Calculate mutual information
            try:
                mi_score = mutual_info_score(feature_values, y_train)
                # Normalize by entropy of labels
                label_entropy = stats.entropy(np.bincount(y_train))
                normalized_mi = mi_score / label_entropy if label_entropy > 0 else 0
                
                if normalized_mi > 0.95:  # Very high correlation
                    leakage_results['feature_leakage'].append(feature_name)
                    leakage_results['details'].append(f"Feature '{feature_name}' has suspiciously high correlation with labels (MI: {normalized_mi:.3f})")
                    print(f"   ‚ö†Ô∏è  Potential feature leakage: {feature_name} (MI: {normalized_mi:.3f})")
            except:
                pass
        
        if not leakage_results['feature_leakage']:
            print("   ‚úÖ No obvious feature leakage detected")
        
        # 3. Check normalization leakage (this method should be called BEFORE normalization)
        print("   Note: Normalization leakage prevented by fitting scaler only on training data")
        
        # 4. Check for temporal patterns that might indicate leakage
        print("   Checking for temporal patterns...")
        # Look for features that might be time-dependent
        temporal_features = [name for name in feature_names if any(keyword in name.lower() for keyword in ['time', 'duration', 'iat', 'flow'])]
        
        if temporal_features:
            print(f"   Found {len(temporal_features)} potentially time-dependent features")
            print("   Ensure data is properly shuffled and not sorted by time")
            leakage_results['details'].append(f"Found {len(temporal_features)} time-dependent features - ensure proper shuffling")
        
        # Create leakage detection summary plot
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Data Leakage Detection Results', fontsize=16, fontweight='bold')
        
        # Train-test contamination
        contamination_data = ['Clean' if contamination_count == 0 else 'Contaminated']
        contamination_counts = [1]
        axes[0,0].bar(contamination_data, contamination_counts, color=['green' if contamination_count == 0 else 'red'])
        axes[0,0].set_title('Train-Test Contamination')
        axes[0,0].set_ylabel('Status')
        
        # Feature leakage count
        axes[0,1].bar(['Suspicious Features'], [len(leakage_results['feature_leakage'])], 
                     color=['green' if len(leakage_results['feature_leakage']) == 0 else 'orange'])
        axes[0,1].set_title('Potential Feature Leakage')
        axes[0,1].set_ylabel('Count')
        
        # Temporal features
        axes[1,0].bar(['Temporal Features'], [len(temporal_features)], color='blue')
        axes[1,0].set_title('Time-Dependent Features')
        axes[1,0].set_ylabel('Count')
        
        # Summary
        summary_text = "Leakage Check Summary:\n\n"
        summary_text += f"‚úÖ Scaler fitted only on training data\n"
        summary_text += f"{'‚ùå' if leakage_results['train_test_contamination'] else '‚úÖ'} Train-test contamination: {'DETECTED' if leakage_results['train_test_contamination'] else 'None'}\n"
        summary_text += f"{'‚ùå' if leakage_results['feature_leakage'] else '‚úÖ'} Feature leakage: {len(leakage_results['feature_leakage'])} suspicious features\n"
        summary_text += f"‚ÑπÔ∏è  Temporal features: {len(temporal_features)} found\n"
        
        axes[1,1].text(0.05, 0.95, summary_text, transform=axes[1,1].transAxes, 
                      fontsize=10, verticalalignment='top', fontfamily='monospace')
        axes[1,1].set_title('Summary')
        axes[1,1].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'data_leakage_detection.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Save detailed leakage report
        with open(self.viz_dir / 'leakage_detection_report.txt', 'w') as f:
            f.write("Data Leakage Detection Report\n")
            f.write("=" * 40 + "\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("LEAKAGE CHECK RESULTS:\n")
            f.write(f"Train-Test Contamination: {'DETECTED' if leakage_results['train_test_contamination'] else 'NONE'}\n")
            f.write(f"Feature Leakage: {len(leakage_results['feature_leakage'])} suspicious features\n")
            f.write(f"Normalization Leakage: PREVENTED (scaler fitted only on training data)\n\n")
            
            if leakage_results['details']:
                f.write("DETAILED FINDINGS:\n")
                for detail in leakage_results['details']:
                    f.write(f"- {detail}\n")
                f.write("\n")
            
            if temporal_features:
                f.write("TEMPORAL FEATURES DETECTED:\n")
                for feature in temporal_features:
                    f.write(f"- {feature}\n")
                f.write("\nRecommendation: Ensure data is properly shuffled and not time-ordered.\n")
        
        print(f"   ‚úÖ Saved leakage detection report to {self.viz_dir}")
        return leakage_results
    
    def process_all_files(self, force_reprocess=False, validate_features=True):
        """
        Process all CSV files in the raw data directory
        Args:
            force_reprocess (bool): If True, reprocess even if files exist
            validate_features (bool): If True, validate feature count
        Returns:
            dict: Processing results including feature count
        """
        print("\nProcessing CIC-IDS2017 dataset files...")
        results = {
            'num_features': 0,
            'files_processed': 0,
            'total_samples': 0
        }

        # Clear processed directory if force_reprocess
        if force_reprocess:
            print("Clearing processed directory...")
            for file in self.processed_dir.glob("*.pkl"):
                file.unlink()
            for file in self.processed_dir.glob("*.csv"):
                file.unlink()

        # Get list of all raw CSV files
        raw_files = list(self.raw_dir.glob("*.csv"))
        if not raw_files:
            raise FileNotFoundError(f"No CSV files found in {self.raw_dir}")

        # Process each file
        all_data = []
        for file_path in raw_files:
            print(f"\nProcessing {file_path.name}...")
            
            # Read CSV
            df = pd.read_csv(file_path)
            initial_cols = len(df.columns)
            
            if validate_features:
                # -1 because one column is the Label
                num_features = initial_cols - 1
                if num_features != config.NUM_FEATURES:
                    print(f"‚ö†Ô∏è  Warning: Found {num_features} features (expected {config.NUM_FEATURES})")
                    print("Features:", list(df.columns))
                results['num_features'] = num_features
            
            # Handle problematic values
            df = self.handle_infinite_values(df)
            
            # Add to combined dataset
            all_data.append(df)
            results['files_processed'] += 1
            results['total_samples'] += len(df)
            
            print(f"‚úì Processed {len(df):,} samples")

        # Combine all data
        print("\nCombining all processed files...")
        combined_df = pd.concat(all_data, axis=0, ignore_index=True)
        num_features = len(combined_df.columns) - 1  # -1 for Label column
        print(f"Total samples: {len(combined_df):,}")
        print(f"Features: {num_features}")
        
        if num_features != config.NUM_FEATURES:
            raise ValueError(f"ERROR: Got {num_features} features after preprocessing, expected {config.NUM_FEATURES}. "
                           f"Some features may have been incorrectly removed.")

        # Split features and labels (note the space in ' Label')
        X = combined_df.drop(' Label', axis=1)
        y = combined_df[' Label']

        # Strip whitespace from column names
        X.columns = X.columns.str.strip()

        # Convert labels to numeric
        self.label_encoder.fit(y)
        y = self.label_encoder.transform(y)

        # Split into train/test sets
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        # Save combined processed data
        output_file = self.processed_dir / "processed_data.pkl"
        with open(output_file, 'wb') as f:
            pickle.dump({
                'X_train': X_train,
                'X_test': X_test,
                'y_train': y_train,
                'y_test': y_test,
                'num_classes': len(np.unique(y)),
                'class_names': self.all_attack_types,
                'feature_names': list(X.columns),
                'num_features': results['num_features'],
                'num_samples': len(combined_df)
            }, f)
        print(f"\n‚úì Saved processed data to {output_file}")
        print(f"   Train samples: {len(X_train):,}")
        print(f"   Test samples: {len(X_test):,}")
        print(f"   Features: {X_train.shape[1]}")
        print(f"   Classes: {len(np.unique(y))}")
        
        return results
    
    def handle_infinite_values(self, df):
        """Handle infinite and problematic values in the dataset"""
        print("üîß Handling infinite and extreme values...")
        
        initial_rows = len(df)
        initial_cols = len(df.columns)
        
        # Validate feature count (should be 78 + 1 label = 79 total)
        expected_total = config.ORIGINAL_FEATURES + 1  # 78 + 1 for label
        if initial_cols != expected_total:
            print(f"‚ö†Ô∏è  Warning: Expected {expected_total} columns (78 features + 1 label), but got {initial_cols}")
            print(f"   Will adjust to target {config.NUM_FEATURES} features")
        
        # Get numeric columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        # Check for infinite values
        inf_counts = {}
        for col in numeric_cols:
            if col.strip() != 'Label':
                inf_count = np.isinf(df[col]).sum()
                if inf_count > 0:
                    inf_counts[col] = inf_count
                
        if inf_counts:
            print(f"   Found infinite values in {len(inf_counts)} columns:")
            for col, count in list(inf_counts.items())[:5]:  # Show first 5
                print(f"     - {col}: {count:,} infinite values")
            if len(inf_counts) > 5:
                print(f"     ... and {len(inf_counts) - 5} more columns")
        
        # Replace infinite values with NaN
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # Handle extreme values (very large numbers that might cause numerical issues)
        extreme_total = 0
        for col in numeric_cols:
            if col in df.columns and col.strip() != 'Label':  # Skip label column
                values = df[col].dropna()
                if len(values) > 0:
                    # Calculate reasonable bounds (99.9th percentile)
                    try:
                        upper_bound = values.quantile(0.999)
                        lower_bound = values.quantile(0.001)
                        
                        # Cap extreme values
                        extreme_mask = (df[col] > upper_bound) | (df[col] < lower_bound)
                        extreme_count = extreme_mask.sum()
                        
                        if extreme_count > 0:
                            df.loc[df[col] > upper_bound, col] = upper_bound
                            df.loc[df[col] < lower_bound, col] = lower_bound
                            extreme_total += extreme_count
                            
                    except Exception as e:
                        print(f"     ‚ö†Ô∏è  Issue processing {col}: {e}")
        
        if extreme_total > 0:
            print(f"   Capped {extreme_total:,} extreme values")
        
        # Apply median imputation for missing values (as specified in requirements)
        print("   Applying median imputation for missing values...")
        nan_filled = 0
        for col in numeric_cols:
            if col in df.columns and col.strip() != 'Label' and df[col].isnull().any():
                null_count = df[col].isnull().sum()
                median_val = df[col].median()
                if not pd.isna(median_val):
                    df[col].fillna(median_val, inplace=True)
                    print(f"     - {col}: filled {null_count:,} missing values with median ({median_val:.6f})")
                else:
                    # If median is NaN, use 0
                    df[col].fillna(0, inplace=True)
                    print(f"     - {col}: filled {null_count:,} missing values with 0 (no valid median)")
                nan_filled += null_count
        
        if nan_filled > 0:
            print(f"   ‚úÖ Median imputation complete: {nan_filled:,} values filled")
        else:
            print(f"   ‚úÖ No missing values found - no imputation needed")
        
        # Clean column names (remove leading/trailing spaces)
        df.columns = df.columns.str.strip()
        
        # CRITICAL: Ensure we have exactly the expected number of features
        # CIC-IDS2017 should have 78 features + 1 label = 79 total columns
        expected_features = config.NUM_FEATURES  # 78
        label_col = None
        
        # Find label column
        for col in df.columns:
            if col.lower().strip() in ['label', ' label']:
                label_col = col
                break
        
        if label_col:
            feature_cols = [col for col in df.columns if col != label_col]
            print(f"   Found label column: '{label_col}'")
            print(f"   Feature columns: {len(feature_cols)} (target: {expected_features})")
        else:
            # If no label column found, assume all columns are features
            feature_cols = list(df.columns)
            print(f"   No label column found, treating all {len(feature_cols)} columns as features")
        
        # Preserve all features - only remove if there are obvious duplicates
        feature_cols = [col for col in df.columns if col != 'Label']
        current_features = len(feature_cols)
        print(f"   Current features after cleaning: {current_features}")
        
        # We want to preserve all 78 original features
        features_removed = []
        
        # Only remove features if we somehow have more than the original 78
        # This could happen due to duplicate columns or data processing errors
        if current_features > config.NUM_FEATURES:
            excess_features = current_features - config.NUM_FEATURES
            print(f"   Found {excess_features} excess features beyond the original 78")
            
            # Look for duplicate or obviously problematic columns
            duplicate_cols = []
            for i, col1 in enumerate(feature_cols):
                for j, col2 in enumerate(feature_cols[i+1:], i+1):
                    if col1 == col2 or col1.strip() == col2.strip():
                        if col2 not in duplicate_cols:
                            duplicate_cols.append(col2)
            
            if duplicate_cols:
                print(f"   Removing {len(duplicate_cols)} duplicate columns...")
                for col in duplicate_cols:
                    if col in df.columns:
                        df = df.drop(columns=[col])
                        features_removed.append(col)
                        print(f"     - Removed duplicate: {col}")
        
        # Final check
        final_feature_cols = [col for col in df.columns if col != 'Label']
        final_feature_count = len(final_feature_cols)
        
        if final_feature_count == config.NUM_FEATURES:
            print(f"   ‚úÖ Perfect: Preserved all {final_feature_count} original features")
        elif final_feature_count < config.NUM_FEATURES:
            missing_features = config.NUM_FEATURES - final_feature_count
            print(f"   ‚ö†Ô∏è  Missing {missing_features} features (have {final_feature_count}/{config.NUM_FEATURES})")
            print(f"   This may be due to constant feature removal in clean_data step")
        else:
            excess_features = final_feature_count - config.NUM_FEATURES
            print(f"   ‚ö†Ô∏è  Have {excess_features} extra features ({final_feature_count}/{config.NUM_FEATURES})")
            print(f"   Will keep all features to preserve maximum information")
        
        final_rows = len(df)
        final_feature_cols = [col for col in df.columns if col != 'Label']
        final_features = len(final_feature_cols)
        
        print(f"   ‚úÖ Cleaned data: {final_rows:,} rows, {final_features} features + 1 label = {len(df.columns)} total columns")
        
        if final_features != config.NUM_FEATURES:
            print(f"   ‚ö†Ô∏è  Final feature count: {final_features} (target: {config.NUM_FEATURES})")
        
        return df
    def identify_file_type(self, filename):
        """Identify what attacks are in a file based on filename"""
        filename_lower = filename.lower()
        
        for key, info in self.filename_to_attacks.items():
            if key.lower() in filename_lower:
                return info
        
        # Default if no match found
        return {
            'attacks': ['BENIGN'],
            'distribution': [1.0]
        }
    
    def find_dataset_files(self):
        """Find available CIC-IDS2017 dataset files"""
        # Look for CIC-IDS2017 files
        cicids_files = []
        
        # Official file patterns
        patterns = [
            "*WorkingHours*.csv",
            "*Afternoon*.csv", 
            "*Morning*.csv",
            "*.pcap_ISCX.csv"
        ]
        
        for pattern in patterns:
            files = list(self.raw_dir.glob(pattern))
            cicids_files.extend(files)
        
        # Remove duplicates
        cicids_files = list(set(cicids_files))
        
        # Look for sample file
        sample_file = self.raw_dir / "sample_cicids2017.csv"
        
        if cicids_files:
            print(f"üìÅ Found {len(cicids_files)} CIC-IDS2017 files:")
            for file in cicids_files:
                size_mb = file.stat().st_size / (1024 * 1024)
                file_info = self.identify_file_type(file.name)
                attacks = ', '.join(file_info['attacks'][:3])  # Show first 3 attacks
                if len(file_info['attacks']) > 3:
                    attacks += f" + {len(file_info['attacks'])-3} more"
                print(f"   - {file.name} ({size_mb:.1f} MB) - {attacks}")
            return cicids_files, "official"
            
        elif sample_file.exists():
            print(f"üìÅ Using sample dataset: {sample_file.name}")
            return [sample_file], "sample"
            
        else:
            print("‚ùå No dataset files found!")
            print("Run 'python data/download_dataset.py' first")
            return [], "none"
    
    def load_data_without_labels(self):
        """Load CIC-IDS2017 dataset and create labels from filenames"""
        files, dataset_type = self.find_dataset_files()
        
        if not files:
            raise FileNotFoundError(
                "No dataset files found. Please run 'python data/download_dataset.py' first"
            )
        
        print(f"üìä Loading {dataset_type} dataset without label columns...")
        
        # For sample data, use as-is
        if dataset_type == "sample":
            df = pd.read_csv(files[0])
            if 'Label' in df.columns:
                print("   ‚úÖ Sample file has labels - using as-is")
                return df
            else:
                print("   ‚ö†Ô∏è  Sample file missing labels - will create synthetic labels")
        
        # Load all files and create labels
        all_dataframes = []
        total_rows = 0
        
        for file_path in files:
            print(f"   Loading {file_path.name}...")
            
            try:
                # Read CSV
                df = pd.read_csv(file_path, low_memory=False)
                
                # Clean column names
                df.columns = df.columns.str.strip()
                
                # Check if file already has labels
                has_labels = False
                label_cols = [col for col in df.columns if 'label' in col.lower()]
                
                if label_cols and not df[label_cols[0]].isna().all():
                    # Use existing labels
                    has_labels = True
                    label_col = label_cols[0]
                    if label_col != 'Label':
                        df = df.rename(columns={label_col: 'Label'})
                    
                    # Remove other label columns if any
                    other_label_cols = [col for col in label_cols if col != label_col]
                    if other_label_cols:
                        df = df.drop(columns=other_label_cols)
                        
                    rows = len(df)
                    print(f"      üìà {rows:,} rows, {len(df.columns)-1} features")
                    print(f"      üè∑Ô∏è  Using existing labels: {dict(zip(*np.unique(df['Label'], return_counts=True)))}")
                
                else:
                    # Remove any empty label columns
                    if label_cols:
                        df = df.drop(columns=label_cols)
                    
                    rows = len(df)
                    print(f"      üìà {rows:,} rows, {len(df.columns)} features")
                    
                    # Create labels based on filename
                    file_info = self.identify_file_type(file_path.name)
                    attacks = file_info['attacks']
                    distribution = file_info['distribution']
                    
                    # Normalize distribution to ensure it sums to 1
                    distribution = np.array(distribution)
                    distribution = distribution / distribution.sum()
                    
                    # Assign labels based on distribution
                    labels = np.random.choice(
                        attacks, 
                        size=rows, 
                        p=distribution
                    )
                    
                    df['Label'] = labels
                    
                    print(f"      üè∑Ô∏è  Added labels: {dict(zip(*np.unique(labels, return_counts=True)))}")
                
                all_dataframes.append(df)
                total_rows += rows
                
            except Exception as e:
                print(f"   ‚ùå Error loading {file_path.name}: {e}")
                continue
        
        if not all_dataframes:
            # Create sample data as fallback
            print("‚ö†Ô∏è  No files loaded successfully. Creating sample data...")
            return self.create_sample_data()
        
        # Combine all dataframes
        print(f"\nüîó Combining {len(all_dataframes)} files...")
        df_combined = pd.concat(all_dataframes, ignore_index=True)
        
        # Handle infinite and problematic values
        df_combined = self.handle_infinite_values(df_combined)
        
        print(f"üìä Combined dataset: {len(df_combined):,} rows, {len(df_combined.columns)} columns")
        print(f"üè∑Ô∏è  Attack distribution:")
        
        label_counts = df_combined['Label'].value_counts()
        for label, count in label_counts.head(10).items():
            percentage = (count / len(df_combined)) * 100
            print(f"   - {label}: {count:,} ({percentage:.1f}%)")
        
        return df_combined
    
    def create_sample_data(self):
        """Create sample data when no real data is available"""
        print("üß™ Creating sample CIC-IDS2017 dataset...")
        
        np.random.seed(42)
        n_samples = 10000
        
        # Create realistic network features
        data = {}
        
        # Network flow features (simplified version of CIC-IDS2017 features)
        feature_names = [
            'Destination_Port', 'Flow_Duration', 'Total_Fwd_Packets', 'Total_Backward_Packets',
            'Total_Length_of_Fwd_Packets', 'Total_Length_of_Bwd_Packets', 'Fwd_Packet_Length_Max',
            'Fwd_Packet_Length_Min', 'Fwd_Packet_Length_Mean', 'Bwd_Packet_Length_Max',
            'Flow_Bytes_s', 'Flow_Packets_s', 'Flow_IAT_Mean', 'Flow_IAT_Std', 'Fwd_IAT_Total'
        ]
        
        # Extend to match expected feature count
        while len(feature_names) < 78:
            feature_names.append(f'Feature_{len(feature_names)}')
        
        # Generate synthetic features
        for i, feature in enumerate(feature_names):
            if 'Port' in feature:
                data[feature] = np.random.choice([22, 53, 80, 443, 993, 995], n_samples)
            elif 'Duration' in feature:
                data[feature] = np.random.exponential(5000, n_samples)
            elif 'Packet' in feature:
                data[feature] = np.random.poisson(20, n_samples)
            elif 'Length' in feature:
                data[feature] = np.random.gamma(2, 1000, n_samples)
            elif 'Bytes' in feature:
                data[feature] = np.random.gamma(3, 2000, n_samples)
            else:
                data[feature] = np.random.normal(0, 1, n_samples)
        
        # Create realistic label distribution
        label_probs = [0.75, 0.03, 0.03, 0.03, 0.03, 0.03, 0.02, 0.02, 0.02, 0.02, 0.01, 0.005, 0.005, 0.005, 0.005]
        labels = np.random.choice(self.all_attack_types, n_samples, p=label_probs)
        
        data['Label'] = labels
        
        df = pd.DataFrame(data)
        
        print(f"   ‚úÖ Created sample dataset: {len(df)} samples, {len(df.columns)-1} features")
        print(f"   üè∑Ô∏è  Sample distribution: {dict(zip(*np.unique(labels, return_counts=True)))}")
        
        return df
    
    def clean_unicode_labels(self, labels):
        """Clean Unicode characters from attack labels"""
        cleaned_labels = []
        for label in labels:
            # Replace common Unicode dashes and characters
            cleaned = str(label).replace('‚Äì', '-').replace('‚Äî', '-').replace('ÔøΩ', '-')
            # Fix known Web Attack labels
            if 'Web Attack' in cleaned and 'ÔøΩ' in cleaned:
                if 'Brute Force' in cleaned:
                    cleaned = 'Web Attack - Brute Force'
                elif 'Sql Injection' in cleaned:
                    cleaned = 'Web Attack - Sql Injection'
                elif 'XSS' in cleaned:
                    cleaned = 'Web Attack - XSS'
            cleaned_labels.append(cleaned)
        return cleaned_labels

    def clean_data(self, df):
        """Clean and prepare the dataset"""
        print("\nüßπ Cleaning data...")
        
        print(f"   Original shape: {df.shape}")
        
        # Clean Unicode characters in labels
        if 'Label' in df.columns:
            original_labels = df['Label'].unique()
            df['Label'] = self.clean_unicode_labels(df['Label'])
            cleaned_labels = df['Label'].unique()
            if len(set(original_labels) - set(cleaned_labels)) > 0:
                print("   Fixed Unicode characters in attack labels")
        
        # Handle infinite values
        print("   Replacing infinite values...")
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].replace([np.inf, -np.inf], np.nan)
        
        # Handle missing values
        missing_cols = df.isnull().sum()
        missing_cols = missing_cols[missing_cols > 0]
        
        if len(missing_cols) > 0:
            print(f"   Found missing values in {len(missing_cols)} columns")
            print(f"   Filling missing values...")
            
            # Fill numeric columns with median
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            numeric_cols = [col for col in numeric_cols if col != 'Label']  # Exclude label
            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
            
            # Fill any remaining missing values
            df = df.fillna(0)
        
        # ULTRA-CONSERVATIVE approach - preserve ALL 78 features
        print("   Checking for completely unusable features (ultra-conservative)...")
        constant_cols = []
        near_constant_cols = []
        
        # Get feature columns only (exclude Label)
        feature_columns = [col for col in df.columns if col.strip().lower() != 'label']
        
        print(f"   Total columns: {len(df.columns)}")
        print(f"   Feature columns: {len(feature_columns)}")
        print(f"   Target features: {config.NUM_FEATURES}")
        
        # Only check for truly constant features (all identical values)
        for col in feature_columns:
            if col in df.columns:
                unique_count = df[col].nunique(dropna=False)  # Include NaN in count
                
                if unique_count <= 1:
                    # Only mark for removal if ALL values are truly identical
                    unique_vals = df[col].dropna().unique()
                    if len(unique_vals) <= 1:
                        constant_cols.append(col)
                        print(f"     - CONSTANT: {col} (all values: {unique_vals[0] if len(unique_vals) > 0 else 'NaN'})")
                elif unique_count <= 2 and df[col].dtype in ['int64', 'float64']:
                    # Report but KEEP near-constant features
                    value_counts = df[col].value_counts()
                    if len(value_counts) == 2:
                        dominance = value_counts.iloc[0] / len(df)
                        near_constant_cols.append((col, unique_count, dominance))
        
        # Force preservation of all 78 features if requested
        if self.force_preserve_all_78:
            print(f"   üîí FORCING PRESERVATION of all 78 features (including {len(constant_cols)} constant features)")
            print(f"   Note: Constant features will be kept but have minimal impact on model training")
            # Don't remove any features - keep everything
        elif constant_cols:
            print(f"   Removing {len(constant_cols)} completely constant features: {constant_cols}")
            df = df.drop(columns=constant_cols)
        else:
            print("   ‚úÖ No completely constant features found - preserving all features")
        
        # Report near-constant features but keep them for maximum information preservation
        if near_constant_cols:
            print(f"   Found {len(near_constant_cols)} near-constant features (preserving them):")
            for col, unique_count, dominance in near_constant_cols[:5]:  # Show first 5
                print(f"     - {col}: {unique_count} unique values, {dominance:.1%} dominance")
            if len(near_constant_cols) > 5:
                print(f"     ... and {len(near_constant_cols) - 5} more")
        
        # CRITICAL: Final feature preservation verification
        feature_cols = [col for col in df.columns if col.strip().lower() != 'label']
        current_feature_count = len(feature_cols)
        target_features = config.NUM_FEATURES
        
        print(f"   üìä Feature preservation status:")
        print(f"     - Current: {current_feature_count} features")
        print(f"     - Target: {target_features} features")
        print(f"     - Difference: {current_feature_count - target_features}")
        
        if current_feature_count == target_features:
            print(f"   ‚úÖ PERFECT: All {target_features} original features preserved!")
        elif current_feature_count >= target_features - 2:
            print(f"   ‚úÖ EXCELLENT: {current_feature_count}/{target_features} features preserved (minimal loss)")
        elif current_feature_count >= target_features - 5:
            print(f"   ‚ö†Ô∏è  GOOD: {current_feature_count}/{target_features} features preserved (acceptable loss)")
        else:
            missing = target_features - current_feature_count
            print(f"   ‚ö†Ô∏è  WARNING: Missing {missing} features ({current_feature_count}/{target_features})")
            print(f"   This suggests the raw data may not have all expected CIC-IDS2017 features")
        
        # If we have more features than expected, that's actually good - keep them all
        if current_feature_count > target_features:
            extra = current_feature_count - target_features
            print(f"   üéâ BONUS: Found {extra} extra features - preserving all for maximum information!")
        
        # Separate features and labels
        feature_cols = [col for col in df.columns if col != 'Label']
        X = df[feature_cols]
        y = df['Label']
        
        print(f"   Final feature shape: {X.shape}")
        print(f"   Features: {len(feature_cols)} columns")
        
        # Encode labels
        print("   Encoding labels...")
        y_encoded = self.label_encoder.fit_transform(y)
        
        print(f"   Classes: {len(self.label_encoder.classes_)}")
        for i, class_name in enumerate(self.label_encoder.classes_):
            count = np.sum(y_encoded == i)
            print(f"      {i}: {class_name} ({count:,} samples)")
        
        # Store actual number of classes in instance (config is read-only)
        self.num_classes = len(self.label_encoder.classes_)
        
        return X, y_encoded
    
    def remove_outliers(self, X, y, max_samples=50000):
        """Remove outliers using Isolation Forest (with memory management)"""
        print("\nüîç Removing outliers...")
        
        if len(X) > max_samples:
            print(f"   Large dataset detected ({len(X):,} samples)")
            print(f"   Using sample of {max_samples:,} for outlier detection...")
            
            # Sample for outlier detection
            sample_idx = np.random.choice(len(X), max_samples, replace=False)
            X_sample = X.iloc[sample_idx]
            
            # Fit on sample
            outlier_mask_sample = self.isolation_forest.fit_predict(X_sample) == 1
            print(f"   Sample outlier rate: {(~outlier_mask_sample).mean()*100:.1f}%")
            
            # Apply to full dataset
            print("   Applying outlier detection to full dataset...")
            
            # Process in chunks to manage memory
            chunk_size = 10000
            outlier_mask = np.ones(len(X), dtype=bool)
            
            for start in range(0, len(X), chunk_size):
                end = min(start + chunk_size, len(X))
                chunk_mask = self.isolation_forest.predict(X.iloc[start:end]) == 1
                outlier_mask[start:end] = chunk_mask
                
                if (start // chunk_size + 1) % 5 == 0:
                    print(f"   Processed {end:,}/{len(X):,} samples...")
        else:
            outlier_mask = self.isolation_forest.fit_predict(X) == 1
        
        X_clean = X[outlier_mask]
        y_clean = y[outlier_mask]
        
        removed = len(X) - len(X_clean)
        print(f"   Removed {removed:,} outliers ({removed/len(X)*100:.1f}%)")
        print(f"   Remaining: {len(X_clean):,} samples")
        
        return X_clean, y_clean
    
    def balance_classes(self, X, y, target_samples_per_class=50000):
        """Balance classes to achieve 1:1 ratio across all classes using SMOTE"""
        print("\n‚öñÔ∏è  Balancing classes to achieve 1:1 ratio across all classes...")
        
        original_counts = np.bincount(y)
        unique_labels, counts = np.unique(y, return_counts=True)
        
        print(f"   Original distribution:")
        total_samples = len(y)
        for label, count in zip(unique_labels, counts):
            class_name = self.label_encoder.classes_[label]
            percentage = (count / total_samples) * 100
            print(f"     {label}: {class_name} - {count:,} samples ({percentage:.2f}%)")
        
        # Calculate target count for 1:1 ratio
        # Use a reasonable target that's not too large for memory constraints
        max_current = max(counts)
        min_current = min(counts)
        
        # Target should be reasonable - not too high to avoid memory issues
        target_count = min(target_samples_per_class, max_current // 2)  # Half of largest class or target
        target_count = max(target_count, 10000)  # But at least 10k samples per class
        
        print(f"   Target samples per class: {target_count:,} (1:1 ratio)")
        
        # Create sampling strategy for perfect 1:1 balance
        sampling_strategy = {}
        
        for label, count in zip(unique_labels, counts):
            class_name = self.label_encoder.classes_[label]
            
            if count < target_count:
                sampling_strategy[label] = target_count
                boost_ratio = target_count / count
                print(f"     {class_name}: {count:,} -> {target_count:,} ({boost_ratio:.1f}x boost)")
            else:
                print(f"     {class_name}: {count:,} -> {target_count:,} (will be downsampled)")
        
        # Handle downsampling for majority classes first
        if any(count > target_count for count in counts):
            print(f"   Downsampling majority classes to {target_count:,} samples...")
            
            # Downsample majority classes
            indices_to_keep = []
            
            for label in unique_labels:
                label_indices = np.where(y == label)[0]
                
                if len(label_indices) > target_count:
                    # Randomly sample target_count indices
                    selected_indices = np.random.choice(label_indices, target_count, replace=False)
                    indices_to_keep.extend(selected_indices)
                else:
                    # Keep all indices for minority classes
                    indices_to_keep.extend(label_indices)
            
            # Apply downsampling
            indices_to_keep = sorted(indices_to_keep)
            X = X.iloc[indices_to_keep] if hasattr(X, 'iloc') else X[indices_to_keep]
            y = y[indices_to_keep]
            
            print(f"   After downsampling: {len(X):,} samples")
        
        # Apply SMOTE to achieve 1:1 ratio
        if sampling_strategy:
            print(f"   Applying SMOTE to balance {len(sampling_strategy)} minority classes...")
            print(f"   Will oversample classes: {[self.label_encoder.classes_[label] for label in sampling_strategy.keys()]}")
            
            # Additional data validation before SMOTE
            print("   Validating data for SMOTE...")
            
            # Check for any remaining infinite or NaN values
            if hasattr(X, 'values'):
                X_values = X.values
            else:
                X_values = X
                
            if np.any(np.isinf(X_values)) or np.any(np.isnan(X_values)):
                print("   ‚ö†Ô∏è  Found remaining infinite/NaN values, cleaning...")
                if hasattr(X, 'replace'):
                    X = X.replace([np.inf, -np.inf], np.nan)
                    X = X.fillna(X.median())
                else:
                    X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)
            
            # Ensure all values are finite
            if hasattr(X, 'values'):
                finite_mask = np.isfinite(X.values).all(axis=1)
                if not finite_mask.all():
                    print(f"   Removing {(~finite_mask).sum()} rows with non-finite values")
                    X = X.loc[finite_mask]
                    y = y[finite_mask]
            
            # Calculate k_neighbors based on smallest class size
            current_counts = np.bincount(y)
            min_class_size = min([current_counts[label] for label in sampling_strategy.keys() if label < len(current_counts)])
            k_neighbors = min(5, max(1, min_class_size - 1))
            
            # Create SMOTE with adjusted parameters for 1:1 balancing
            try:
                # Ensure k_neighbors is an integer
                k_neighbors_int = int(k_neighbors)
                print(f"   Running SMOTE for 1:1 class balance (k_neighbors={k_neighbors_int})...")
                
                # Create SMOTE instance with proper type handling
                smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=k_neighbors_int)  # type: ignore
                
                # Apply SMOTE
                result = smote.fit_resample(X, y)
                X_balanced = result[0]
                y_balanced = result[1]
                
                # Verify perfect balance
                balanced_counts = np.bincount(y_balanced)
                print(f"   ‚úÖ SMOTE successful! Achieved 1:1 balance:")
                for label, count in enumerate(balanced_counts):
                    if label < len(self.label_encoder.classes_):
                        class_name = self.label_encoder.classes_[label]
                        print(f"     {class_name}: {count:,} samples")
                        
            except ValueError as e:
                if "k_neighbors" in str(e).lower():
                    print(f"   ‚ö†Ô∏è  SMOTE k_neighbors error - trying with k_neighbors=1...")
                    try:
                        smote_fallback = SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=1)  # type: ignore
                        result = smote_fallback.fit_resample(X, y)
                        X_balanced = result[0]
                        y_balanced = result[1]
                        print("   ‚úÖ SMOTE successful with k_neighbors=1")
                    except Exception as e2:
                        print(f"   ‚ùå SMOTE failed: {e2}")
                        print("   Continuing without perfect balancing...")
                        X_balanced, y_balanced = X, y
                else:
                    print(f"   ‚ö†Ô∏è  SMOTE failed: {e}")
                    print("   Continuing without perfect balancing...")
                    X_balanced, y_balanced = X, y
            except Exception as e:
                print(f"   ‚ö†Ô∏è  SMOTE failed with unexpected error: {e}")
                print("   Continuing without perfect balancing...")
                X_balanced, y_balanced = X, y
        else:
            print("   No minority classes need oversampling")
            X_balanced, y_balanced = X, y
        
        # Final balance verification
        final_counts = np.bincount(y_balanced)
        print(f"\n   Final class distribution after balancing:")
        for label, count in enumerate(final_counts):
            if label < len(self.label_encoder.classes_):
                class_name = self.label_encoder.classes_[label]
                percentage = (count / len(y_balanced)) * 100
                print(f"     {class_name}: {count:,} samples ({percentage:.1f}%)")
        
        return X_balanced, y_balanced
    
    def split_data(self, X, y):
        """Split data with proper shuffling and deduplication to prevent leakage"""
        print("\n‚úÇÔ∏è  Splitting data (before scaling to prevent leakage)...")
        
        # Step 1: Remove duplicate rows to prevent train-test contamination
        print("   Removing duplicate samples to prevent train-test contamination...")
        if hasattr(X, 'values'):
            X_array = X.values
        else:
            X_array = X
        
        # Create DataFrame for deduplication
        combined_data = pd.DataFrame(X_array)
        combined_data['target'] = y
        
        initial_samples = len(combined_data)
        combined_data = combined_data.drop_duplicates()
        final_samples = len(combined_data)
        duplicates_removed = initial_samples - final_samples
        
        if duplicates_removed > 0:
            print(f"   Removed {duplicates_removed:,} duplicate samples ({duplicates_removed/initial_samples*100:.1f}%)")
        else:
            print("   No duplicate samples found")
        
        # Extract features and labels
        X_deduplicated = combined_data.drop('target', axis=1).values
        y_deduplicated = combined_data['target'].values
        
        # Step 2: Split data with stratification and shuffling to prevent temporal leakage
        print("   Splitting with stratification and shuffling to prevent temporal leakage...")
        X_train, X_test, y_train, y_test = train_test_split(
            X_deduplicated, y_deduplicated, 
            test_size=0.3, 
            random_state=42, 
            stratify=np.array(y_deduplicated),
            shuffle=True  # Enable shuffling for stratified split
        )
        
        print(f"   Training set: {X_train.shape} ({len(X_train):,} samples)")
        print(f"   Test set: {X_test.shape} ({len(X_test):,} samples)")
        
        return X_train, X_test, y_train, y_test
    
    def normalize_features(self, X_train, X_test):
        """Z-score normalization (Œº=0, œÉ=1) - fit only on training data to prevent data leakage"""
        print("\nüìä Applying z-score normalization (Œº=0, œÉ=1) - preventing data leakage...")
        
        # Convert to numpy if pandas DataFrame
        if hasattr(X_train, 'values'):
            X_train_array = X_train.values
            X_test_array = X_test.values
            feature_names = X_train.columns.tolist()
        else:
            X_train_array = X_train
            X_test_array = X_test
            feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]
        
        # CRITICAL: Fit scaler ONLY on training data to prevent data leakage
        print("   Fitting scaler on training data only...")
        X_train_scaled = self.scaler.fit_transform(X_train_array)
        
        # Transform test data using the scaler fitted on training data
        print("   Transforming test data using training statistics...")
        X_test_scaled = self.scaler.transform(X_test_array)
        
        # Verify no data leakage in scaling
        train_mean = X_train_scaled.mean(axis=0)
        train_std = X_train_scaled.std(axis=0)
        test_mean = X_test_scaled.mean(axis=0)
        
        print(f"   Normalized {X_train_scaled.shape[1]} features")
        print(f"   Training set mean: {train_mean.mean():.6f} (should be ~0)")
        print(f"   Training set std: {train_std.mean():.6f} (should be ~1)")
        print(f"   Test set mean: {test_mean.mean():.6f} (will differ from 0 - this is expected)")
        
        # Check for potential scaling issues
        if np.any(np.isnan(X_train_scaled)) or np.any(np.isnan(X_test_scaled)):
            print("   ‚ö†Ô∏è  Warning: NaN values found after scaling")
        
        if np.any(np.isinf(X_train_scaled)) or np.any(np.isinf(X_test_scaled)):
            print("   ‚ö†Ô∏è  Warning: Infinite values found after scaling")
        
        return X_train_scaled, X_test_scaled, feature_names
    
    def save_processed_data(self, X_train, X_test, y_train, y_test, feature_names):
        """Save processed data"""
        print("\nüíæ Saving processed data...")
        
        data = {
            'X_train': X_train,
            'X_test': X_test,
            'y_train': y_train,
            'y_test': y_test,
            'label_encoder': self.label_encoder,
            'scaler': self.scaler,
            'feature_names': feature_names,
            'num_classes': len(self.label_encoder.classes_),
            'class_names': self.label_encoder.classes_.tolist()
        }
        
        processed_file = self.processed_dir / "processed_data.pkl"
        with open(processed_file, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"   ‚úÖ Saved: {processed_file}")
        
        # Save summary
        summary = {
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'features': X_train.shape[1],
            'classes': len(self.label_encoder.classes_),
            'class_names': self.label_encoder.classes_.tolist()
        }
        
        print(f"   üìä Summary: {summary}")
        return processed_file
    
    def preprocess_full_pipeline(self):
        """Complete preprocessing pipeline with visualizations and data leakage detection"""
        print("üöÄ Enhanced CIC-IDS2017 Preprocessing")
        print("=" * 70)
        print("Features: Visualizations, Data Leakage Detection, Proper Scaling")
        print("=" * 70)
        
        try:
            # Step 1: Load data and create labels from filenames
            print("\nüìÅ Step 1: Loading raw data...")
            df = self.load_data_without_labels()
            
            # Step 1.5: Create BEFORE preprocessing visualizations
            print("\nüìä Step 1.5: Creating before-preprocessing visualizations...")
            self.create_before_preprocessing_visualizations(df)
            
            # Step 2: Clean data and handle missing values with imputation tracking
            print("\nüßπ Step 2: Cleaning data...")
            original_df = df.copy()  # Keep original for imputation validation
            X, y = self.clean_data(df)
            
            # Validate that we preserved the original 78 features
            feature_count = X.shape[1]
            target_features = config.NUM_FEATURES  # Should be 78
            
            if feature_count == target_features:
                print(f"‚úÖ Perfect: Preserved all {feature_count} original features")
            elif feature_count >= target_features - 2:
                print(f"‚úÖ Excellent: Have {feature_count}/{target_features} features (minimal loss)")
            elif feature_count >= target_features - 5:
                print(f"‚ö†Ô∏è  Good: Have {feature_count}/{target_features} features (small loss)")
            else:
                print(f"‚ö†Ô∏è  Warning: Only {feature_count}/{target_features} features preserved")
                print("   Some original features may have been lost during cleaning")
            
            # Store feature names before splitting (split_data returns numpy arrays)
            feature_names = X.columns.tolist() if hasattr(X, 'columns') else [f'feature_{i}' for i in range(X.shape[1])]
            
            # Step 3: Split data BEFORE any other processing to prevent leakage
            print("\n‚úÇÔ∏è  Step 3: Splitting data (before scaling to prevent leakage)...")
            X_train, X_test, y_train, y_test = self.split_data(X, y)
            
            # Step 3.5: Data leakage detection (BEFORE normalization)
            print("\nüîç Step 3.5: Detecting data leakage...")
            leakage_results = self.detect_data_leakage(X_train, X_test, y_train, y_test, feature_names)
            
            # Step 4: Remove outliers from training data only
            print("\nüéØ Step 4: Removing outliers (training data only)...")
            # Convert numpy array back to DataFrame for outlier detection
            X_train_df = pd.DataFrame(X_train, columns=feature_names)
            X_train_clean, y_train_clean = self.remove_outliers(X_train_df, y_train)
            
            # Step 5: Balance classes
            print("\n‚öñÔ∏è  Step 5: Balancing classes...")
            X_train_balanced, y_train_balanced = self.balance_classes(X_train_clean, y_train_clean)
            
            # Step 6: Create imputation validation (if there was missing data)
            if original_df.isnull().sum().sum() > 0:
                print("\nüî¨ Step 6: Validating imputation...")
                X_train_array = X_train_balanced.values if hasattr(X_train_balanced, 'values') else X_train_balanced
                self.create_imputation_validation_plots(original_df, X_train_array, feature_names)
            
            # Step 7: Normalize features (CRITICAL: fit only on training data)
            print("\nüìä Step 7: Normalizing features (preventing data leakage)...")
            # Convert test data to DataFrame for consistency
            X_test_df = pd.DataFrame(X_test, columns=feature_names)
            X_train_final, X_test_final, feature_names_final = self.normalize_features(X_train_balanced, X_test_df)
            
            # Step 8: Create AFTER preprocessing visualizations
            print("\nüìä Step 8: Creating after-preprocessing visualizations...")
            self.create_after_preprocessing_visualizations(X_train_final, X_test_final, y_train_balanced, y_test, feature_names_final)
            
            # Step 9: Final validation
            print("\nüîç Step 9: Final validation...")
            
            final_feature_count = X_train_final.shape[1]
            target_features = config.NUM_FEATURES  # Should be 78
            
            # Validate we have the complete 78 feature set
            if final_feature_count == target_features:
                print(f"   ‚úÖ Perfect: All {final_feature_count} original features preserved")
            elif final_feature_count >= target_features - 2:
                print(f"   ‚úÖ Excellent: {final_feature_count}/{target_features} features (minimal loss)")
            else:
                print(f"   ‚ö†Ô∏è  Have {final_feature_count}/{target_features} features")
                print("   Some features were lost but continuing with available information")
            
            # Check for data issues
            if np.any(np.isnan(X_train_final)) or np.any(np.isnan(X_test_final)):
                raise ValueError("NaN values found in final processed data")
            
            if np.any(np.isinf(X_train_final)) or np.any(np.isinf(X_test_final)):
                raise ValueError("Infinite values found in final processed data")
            
            print("   ‚úÖ Data quality validation passed")
            print(f"   ‚úÖ Final dataset: {final_feature_count} features, {len(X_train_final):,} train samples, {len(X_test_final):,} test samples")
            
            # Step 10: Save processed data
            print("\nüíæ Step 10: Saving processed data...")
            output_file = self.save_processed_data(
                X_train_final, X_test_final, 
                y_train_balanced, y_test,
                feature_names_final
            )
            
            # Final summary
            print("\n" + "=" * 70)
            print("‚úÖ ENHANCED PREPROCESSING COMPLETE!")
            print("=" * 70)
            final_features = X_train_final.shape[1]
            target_features = config.NUM_FEATURES
            
            print(f"üìä Final Statistics:")
            if final_features == target_features:
                print(f"   Features: {final_features}/{target_features} ‚úÖ (All original features preserved)")
            else:
                print(f"   Features: {final_features}/{target_features} ({'‚úÖ' if final_features >= target_features - 2 else '‚ö†Ô∏è'}) ")
            print(f"   Training: {X_train_final.shape} ({len(y_train_balanced):,} samples)")
            print(f"   Test: {X_test_final.shape} ({len(y_test):,} samples)")
            print(f"   Classes: {len(self.label_encoder.classes_)}")
            print(f"   Feature range: [{X_train_final.min():.3f}, {X_train_final.max():.3f}]")
            print(f"   Output: {output_file}")
            
            print(f"\nüìä Data Leakage Check:")
            print(f"   Train-test contamination: {'‚ùå DETECTED' if leakage_results.get('train_test_contamination', False) else '‚úÖ None'}")
            print(f"   Feature leakage: {'‚ùå' if leakage_results.get('feature_leakage', []) else '‚úÖ'} {len(leakage_results.get('feature_leakage', []))} suspicious features")
            print(f"   Normalization leakage: ‚úÖ Prevented (scaler fit only on training data)")
            
            print(f"\nüìà Visualizations saved to: {self.viz_dir}")
            print(f"   - raw_data_overview.png")
            print(f"   - raw_feature_distributions.png") 
            print(f"   - raw_correlation_matrix.png")
            print(f"   - preprocessed_data_overview.png")
            print(f"   - feature_importance.png")
            print(f"   - data_leakage_detection.png")
            if original_df.isnull().sum().sum() > 0:
                print(f"   - imputation_validation.png")
            
            print("\nüöÄ Ready for model training!")
            print("   Next: python experiments/train_baseline.py")
            
            return X_train_final, X_test_final, y_train_balanced, y_test
            
        except Exception as e:
            print(f"\n‚ùå Enhanced preprocessing failed: {e}")
            print("\nüîß Troubleshooting:")
            print("   1. Check if files exist in data/raw/")
            print("   2. Ensure you have matplotlib, seaborn installed")
            print("   3. Check available memory (large dataset)")
            print("   4. Review logs above for specific errors")
            print("   5. Verify CIC-IDS2017 dataset has correct structure (78 features + 1 label)")
            import traceback
            traceback.print_exc()
            raise

def main():
    """Run preprocessing pipeline"""
    preprocessor = CICIDSPreprocessor()
    
    try:
        X_train, X_test, y_train, y_test = preprocessor.preprocess_full_pipeline()
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")

if __name__ == "__main__":
    main()
